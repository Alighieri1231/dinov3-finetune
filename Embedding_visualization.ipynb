{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d6019f4-8a11-4841-9c4f-2cafbbee4c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import json\n",
    "import glob\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "356d9b8f-a256-421b-88ed-3aecf99ed5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow for processing videos, and images of different input sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7e3a444-55d9-4daf-b063-a9fb680bdeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/rob/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained ViT model\n",
    "size = \"small\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "backbones = {\n",
    "    \"small\": \"vits14_reg\",\n",
    "    \"base\": \"vitb14_reg\",\n",
    "    \"large\": \"vitl14_reg\",\n",
    "    \"giant\": \"vitg14_reg\",\n",
    "}\n",
    "\n",
    "# We will use this encoder in the rest of the notebook\n",
    "encoder = torch.hub.load(repo_or_dir=\"facebookresearch/dinov2\", model=f\"dinov2_{backbones[size]}\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5638ff-f56a-4237-9459-fba3d5a78d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720 1280\n"
     ]
    }
   ],
   "source": [
    "# Load an example video\n",
    "def extract_frames(video_path) -> list[np.ndarray]:\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    video.release()\n",
    "    return frames\n",
    "\n",
    "def closest_multiple(val, multiple):\n",
    "    return int(multiple * round(val / multiple))\n",
    "    \n",
    "# Preprocess all the input image(s)\n",
    "def transform(img, patch_size):\n",
    "    height, width, _ = img.shape\n",
    "\n",
    "    # Conform to the patch size of the ViT\n",
    "    div_height = closest_multiple(height, patch_size)\n",
    "    div_width = closest_multiple(width, patch_size)\n",
    "    img = cv2.resize(img, (div_width, div_height))\n",
    "\n",
    "    # Determine the 0 padding for non-square input\n",
    "    height, width, _ = img.shape\n",
    "    max_dim = max(height, width)\n",
    "    \n",
    "    # Calculate padding for each side\n",
    "    top = (max_dim - height) // 2\n",
    "    bot = max_dim - height - top\n",
    "    left = (max_dim - width) // 2\n",
    "    right = max_dim - width - left\n",
    "    img = cv2.copyMakeBorder(img, top, bot, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
    "    \n",
    "    # Normalize the input\n",
    "    img = (img - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "    img /= 255\n",
    "\n",
    "    # Move the channel dim\n",
    "    img = img.transpose(2, 0, 1)\n",
    "    return img, (div_height, div_width)\n",
    "    \n",
    "file_path = \"koala.mp4\"\n",
    "frames = extract_frames(file_path)\n",
    "\n",
    "original_height, original_width, _ = frames[0].shape\n",
    "print(original_height, original_width)\n",
    "\n",
    "# Preprocess all the frames\n",
    "processed_frames = [transform(frame, encoder.patch_size)[0] for frame in frames]\n",
    "frame_dim = transform(frames[0], encoder.patch_size)[1]\n",
    "\n",
    "# To resize the patches with\n",
    "frame_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d636748-75f2-4910-86a1-30a0035dcc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_features(features, encoder, frame_dim, seperate_foreground):\n",
    "    patches = int(features.shape[1]**0.5)\n",
    "    features = features.reshape(1, patches, patches, -1)\n",
    "\n",
    "    # Crop the feature patches if the input was not square\n",
    "    start_x, end_x = 0, patches\n",
    "    start_y, end_y = 0, patches\n",
    "    if frame_dim[0] > frame_dim[1]:\n",
    "        # first dim x\n",
    "        excess_patches = (frame_dim[0] - frame_dim[1]) / encoder.patch_size\n",
    "        excess_patch = math.ceil(excess_patches / 2) \n",
    "        start_x, end_x = int(excess_patch), int(patches - excess_patch)\n",
    "    elif frame_dim[0] < frame_dim[1]:\n",
    "        # second dim y\n",
    "        excess_patches = (frame_dim[1] - frame_dim[0]) / encoder.patch_size\n",
    "        excess_patch = math.ceil(excess_patches / 2)\n",
    "        start_y, end_y = int(excess_patch), int(patches - excess_patch)\n",
    "    cropped_features = features[:, start_y:end_y, start_x:end_x]\n",
    "\n",
    "    # Continue with cropped features\n",
    "    \n",
    "    patch_features = cropped_features.reshape((end_x - start_x) * (end_y - start_y), -1)\n",
    "    \n",
    "    # Apply PCA and MinMaxScaler\n",
    "    pca = PCA(n_components=3)\n",
    "    scaler = MinMaxScaler(clip=True)\n",
    "    pca.fit(patch_features)\n",
    "    pca_features = pca.transform(patch_features)\n",
    "    scaler.fit(pca_features)\n",
    "    pca_features = scaler.transform(pca_features)\n",
    "\n",
    "    # Separate background and foreground\n",
    "    if seperate_foreground:\n",
    "        pca_background = pca_features[:, 0] > threshold\n",
    "        pca_foreground = ~pca_background\n",
    "    \n",
    "        # Refit PCA for foreground\n",
    "        pca.fit(patch_features[pca_foreground])\n",
    "        pca_features_rem = pca.transform(patch_features[pca_foreground])\n",
    "        scaler.fit(pca_features_rem)\n",
    "        pca_features_rem = scaler.transform(pca_features_rem)\n",
    "\n",
    "        pca_features_rgb = np.zeros(((end_x - start_x) * (end_y - start_y), 3))\n",
    "        pca_features_rgb[pca_background] = 0\n",
    "        pca_features_rgb[pca_foreground] = pca_features_rem\n",
    "        pca_features = pca_features_rgb.reshape((end_y - start_y), (end_x - start_x), 3)\n",
    "\n",
    "    # Convert the PCA features to an image\n",
    "    pca_features = pca_features.reshape((end_y - start_y), (end_x - start_x), 3)\n",
    "    return pca_features\n",
    "\n",
    "def visualize_frames(\n",
    "    frames,\n",
    "    encoder,\n",
    "    image_dim=(),\n",
    "    frame_dim=(),\n",
    "    seperate_foreground=True\n",
    "):\n",
    "    output_frames = []\n",
    "    for frame in frames:\n",
    "        x = torch.tensor(frame).to(device).unsqueeze(0).float()\n",
    "        \n",
    "        # Extract features using the ViT model\n",
    "        features_dict = encoder.forward_features(x)\n",
    "        features = features_dict[\"x_norm_patchtokens\"].detach().cpu().numpy()\n",
    "        \n",
    "        feature_frame = visualize_features(\n",
    "            features,\n",
    "            encoder,\n",
    "            frame_dim,\n",
    "            seperate_foreground\n",
    "        )\n",
    "\n",
    "        # Reshape to the original frame size\n",
    "        feature_frame = (feature_frame * 255).astype(np.uint8)\n",
    "        # Important, cv2 switches location of width and height\n",
    "        height, width = image_dim \n",
    "        feature_frame = cv2.resize(feature_frame, (width, height))\n",
    "        output_frames.append(feature_frame)\n",
    "    return output_frames\n",
    "\n",
    "output_frames = visualize_frames(\n",
    "    processed_frames,\n",
    "    encoder,\n",
    "    (original_height, original_width),\n",
    "    frame_dim,\n",
    "    seperate_foreground=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d001ceb2-150a-4da5-bd4f-9d435604e902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now convert the processed frames to video\n",
    "# For this I used an extra package.\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "clip = ImageSequenceClip(output_frames, fps=20)\n",
    "clip.write_videofile('output.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f62274-fabc-4775-84fa-efd44ec5b40f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
